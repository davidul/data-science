{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivatives\n",
    "\n",
    "Symbolic derivatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sympy as sym\n",
    "import sympy.plotting.plot as symplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sym.symbols('x')\n",
    "fx = 2*x**2\n",
    "\n",
    "dx = sym.diff(fx, x)\n",
    "print(fx)\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symplot(fx,(x,-4,4), title='Function')\n",
    "plt.show()\n",
    "\n",
    "symplot(dx, (x,4,-4), title='Derivative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RELU and Sigmoid activation functions\n",
    "relu = sym.Max(0,x)\n",
    "sigmoid = 1 / (1+sym.exp(-x))\n",
    "\n",
    "# Function plot\n",
    "p = symplot(relu, (x, 4, -4), label='ReLU', show=False, line_color='blue')\n",
    "p.extend(symplot(sigmoid, (x,4,-4), label='Sigmoid', show=False, line_color='red'))\n",
    "p.legend = True\n",
    "p.title = 'Activation Functions'\n",
    "p.show()\n",
    "\n",
    "# Derivative plot\n",
    "\n",
    "p = symplot(sym.diff(relu), (x,-4,4), label='df(ReLU)', show=False, line_color='blue')\n",
    "p.extend(symplot(sym.diff(sigmoid), (x, -4,4), label='df(Sigmoid)', show=False, line_color='red'))\n",
    "p.legend = True\n",
    "p.title = 'Derivatives'\n",
    "p.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules\n",
    "\n",
    "Addition\n",
    "$$(f + g)' = f' + g'$$\n",
    "\n",
    "Multiplication\n",
    "$$ (f \\cdot g)' = f' \\cdot g + f \\cdot g'$$\n",
    "\n",
    "Chain rules\n",
    "$$ \\frac{df}{dx} f(g(x)) = f'(g(x))g'(x)$$\n",
    "\n",
    "Example\n",
    "$$ \\frac{df}{dx}(x^2 + 4x^3)^5 = 5(x^2 + 4x^3)^4 (2x + 12x^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sym\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "x = sym.symbols('x')\n",
    "\n",
    "fx = 2*x**2\n",
    "gx = 4*x**3 - 3*x**4\n",
    "\n",
    "df = sym.diff(fx)\n",
    "dg = sym.diff(gx)\n",
    "\n",
    "# product rule manual\n",
    "manual = df * gx + fx * dg\n",
    "\n",
    "viasympy = sym.diff(fx*dx)\n",
    "\n",
    "print('The functions:')\n",
    "display(fx)\n",
    "display(gx)\n",
    "\n",
    "print('Derivatives:')\n",
    "display(df)\n",
    "display(dg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain rule\n",
    "\n",
    "fx = (x**2 + 4*x**3)**5\n",
    "\n",
    "print('The functions:')\n",
    "display(fx)\n",
    "print(' ')\n",
    "print('Derivative:')\n",
    "display(sym.diff(fx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Rule\n",
    "\n",
    "Derivative of a composite function.\n",
    "$$ \\frac{df}{dx} f(g(x)) = f'(g(x))g'(x)$$\n",
    "\n",
    "If we have two functions `sigmoid` and `square` such that `sigmoid(x) = 1 / (1 + exp(-x))` and `square(x) = x * x`, then the derivative of `sigmoid(square(x))` is `sigmoid'(square(x)) * square'(x)`.\n",
    "\n",
    "It is like applying the functions sequentially and taking the derivative of the result. \n",
    "\n",
    "1. Calculate square(x)\n",
    "2. Calculate square'(x)\n",
    "3. Calculate sigmoid'(square(x)), use step 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chain_rule import chain_derivative, Chain, sigmoid,square, chain_length_2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Callable\n",
    "\n",
    "# chain of functions\n",
    "\n",
    "# square(sigmoid)\n",
    "c1: List[Chain] = [sigmoid, square]\n",
    "# sigmoid(square)\n",
    "c2: List[Chain] = [square, sigmoid]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,8))\n",
    "# square(sigmoid)\n",
    "ax[0].plot(np.arange(-4,4,0.1), chain_length_2(c1, np.arange(-4,4,0.1)), label='f(x)')\n",
    "ax[0].plot(np.arange(-4,4,0.1), chain_derivative(c1, np.arange(-4,4,0.1)), label='df(x)')\n",
    "ax[0].set_title('square(sigmoid)')\n",
    "ax[0].legend()\n",
    "\n",
    "# sigmoid(square)\n",
    "ax[1].plot(np.arange(-4,4,0.1), chain_length_2(c2, np.arange(-4,4,0.1)), label='f(x)')\n",
    "ax[1].plot(np.arange(-4,4,0.1), chain_derivative(c2, np.arange(-4,4,0.1)), label='df(x)')\n",
    "ax[1].legend()\n",
    "ax[1].set_title('sigmoid(square)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Leaky relu](../../06-ann/01-ann.ipynb#Leaky-ReLU) is a composite function of `max` and `linear`. The derivative of `leaky_relu(linear(x))` is `leaky_relu'(linear(x)) * linear'(x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chain_rule import chain_derivative, Chain, sigmoid,square, chain_length_3, leaky_relu, chain_deriv_3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Callable\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,8))\n",
    "\n",
    "chain_1:List[Chain] = [leaky_relu, square, sigmoid]\n",
    "chain_2:List[Chain] = [leaky_relu, sigmoid, square]\n",
    "\n",
    "ax[0].plot(np.arange(-4,4,0.1), chain_length_3(chain_1, np.arange(-4,4,0.1)), label='f(x)')\n",
    "ax[0].plot(np.arange(-4,4,0.1), chain_deriv_3(chain_1, np.arange(-4,4,0.1)), label='df(x)')\n",
    "ax[0].set_title('leaky_relu(square(sigmoid))')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(np.arange(-4,4,0.1), chain_length_3(chain_2, np.arange(-4,4,0.1)), label='f(x)')\n",
    "ax[1].plot(np.arange(-4,4,0.1), chain_deriv_3(chain_2, np.arange(-4,4,0.1)), label='df(x)')\n",
    "ax[1].set_title('leaky_relu(sigmoid(square))')\n",
    "ax[1].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted sum\n",
    "Vector of features $x$ and vector of weights $w$.\n",
    "$$ S = \\nu(x,w)= \\sum_i x_i w_i$$\n",
    "\n",
    "What is the derivative of $S$ with respect to $x_i$ $\\frac{\\partial \\nu}{\\partial x_i}$? What is the derivative of $S$ with respect to $w_i$ \n",
    "$\\frac{\\partial \\nu}{\\partial w_i}$?\n",
    "\n",
    "$\\frac{\\partial \\nu}{\\partial X} = [\\frac{\\partial \\nu}{\\partial x_1} \\frac{\\partial \\nu}{\\partial x_2} \\frac{\\partial \\nu}{\\partial x_3}]$\n",
    "\n",
    "$\\frac{\\partial \\nu}{\\partial X} = [w_1 w_2 w_3]$\n",
    "\n",
    "Derivative of weighted sum with respect to $x_i$ is $w_i$.\n",
    "\n",
    "## Activation function\n",
    "\n",
    "$$ s = f(X,W) = \\sigma(\\nu(X,W)) = \\sigma(x_1 \\cdot w_1 + x_2 \\cdot w_2 + x_3 \\cdot w_3) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chain_rule import matrix_function_backward_1, sigmoid\n",
    "import numpy as np\n",
    "X = np.transpose(np.array([[1,2,3,4]]))\n",
    "W = np.array([[0.5,0.5,0.5,0.5]])\n",
    "print(X.shape[1])\n",
    "print(W.shape[0])\n",
    "print(matrix_function_backward_1(X, W, sigmoid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions with multiple inputs\n",
    "\n",
    "The chain rule for a function with two inputs can be applied when you have a function composed of other functions and you want to find the derivative of the composite function with respect to one of the inputs.\n",
    "\n",
    "In the context of the function you provided, where $f(x,y) = \\sigma(\\alpha)$ and $\\alpha(x,y) = x + y$, the chain rule can be applied to find the derivative of $f$ with respect to $x$ as follows:\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial x} = \\frac{\\partial \\sigma}{\\partial \\alpha} \\cdot \\frac{\\partial \\alpha}{\\partial x} $$\n",
    "\n",
    "Here, $\\frac{\\partial \\sigma}{\\partial \\alpha}$ is the derivative of the function $\\sigma$ with respect to $\\alpha$, and $\\frac{\\partial \\alpha}{\\partial x}$ is the derivative of the function $\\alpha$ with respect to $x$.\n",
    "\n",
    "The derivative $\\frac{\\partial \\alpha}{\\partial x}$ is straightforward to compute because $\\alpha(x,y) = x + y$, so its derivative with respect to $x$ is 1.\n",
    "\n",
    "The derivative $\\frac{\\partial \\sigma}{\\partial \\alpha}$ depends on the specific form of the function $\\sigma$. If $\\sigma$ is a sigmoid function, for example, its derivative with respect to its input can be expressed in terms of the sigmoid function itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chain_rule import multiple_inputs_add, sigmoid, Chain, multiple_inputs_add_backward\n",
    "from typing import List, Callable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.arange(-4,4,.1)\n",
    "Y = np.arange(0,8,.1)\n",
    "\n",
    "R = multiple_inputs_add(X, Y, sigmoid)\n",
    "dsda, dadx, dady = multiple_inputs_add_backward(X, Y, sigmoid)\n",
    "\n",
    "fig,axis = plt.subplots(nrows=1, ncols=1, figsize=(12,8))\n",
    "axis.plot(np.arange(-4,4,0.1), R, label='f(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions with multiple vector inputs\n",
    "\n",
    "Matrix representation\n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "x_{11} & x_{12} & x_{13} \\\\\n",
    "x_{21} & x_{22} & x_{23} \\\\\n",
    "x_{31} & x_{32} & x_{33} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "w_{11} & w_{12} \\\\\n",
    "w_{21} & w_{22} \\\\\n",
    "w_{31} & w_{32} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$ X \\cdot W = \\begin{bmatrix}\n",
    "x_{11} \\cdot w_{11} + x_{12} \\cdot w_{21} + x_{13} \\cdot w_{31} & x_{11} \\cdot w_{12} + x_{12} \\cdot w_{22} + x_{13} \\cdot w_{32} \\\\\n",
    "x_{21} \\cdot w_{11} + x_{22} \\cdot w_{21} + x_{23} \\cdot w_{31} & x_{21} \\cdot w_{12} + x_{22} \\cdot w_{22} + x_{23} \\cdot w_{32} \\\\\n",
    "x_{31} \\cdot w_{11} + x_{32} \\cdot w_{21} + x_{33} \\cdot w_{31} & x_{31} \\cdot w_{12} + x_{32} \\cdot w_{22} + x_{33} \\cdot w_{32} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Feed the result throuh $\\sigma$ function. Apply $\\sigma$ to each element of the matrix.\n",
    "$$ \\sigma(X \\cdot W) = \\begin{bmatrix}\n",
    "\\sigma(x_{11} \\cdot w_{11} + x_{12} \\cdot w_{21} + x_{13} \\cdot w_{31}) & \\sigma(x_{11} \\cdot w_{12} + x_{12} \\cdot w_{22} + x_{13} \\cdot w_{32}) \\\\\n",
    "\\sigma(x_{21} \\cdot w_{11} + x_{22} \\cdot w_{21} + x_{23} \\cdot w_{31}) & \\sigma(x_{21} \\cdot w_{12} + x_{22} \\cdot w_{22} + x_{23} \\cdot w_{32}) \\\\\n",
    "\\sigma(x_{31} \\cdot w_{11} + x_{32} \\cdot w_{21} + x_{33} \\cdot w_{31}) & \\sigma(x_{31} \\cdot w_{12} + x_{32} \\cdot w_{22} + x_{33} \\cdot w_{32}) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Gradient of L with respect to X and W.\n",
    "\n",
    "$$  \\frac{\\partial L}{\\partial u}(X) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial L}{\\partial u}(x_{11}) & \\frac{\\partial L}{\\partial u}(x_{12}) & \\frac{\\partial L}{\\partial u}(x_{13}) \\\\\n",
    "\\frac{\\partial L}{\\partial u}(x_{21}) & \\frac{\\partial L}{\\partial u}(x_{22}) & \\frac{\\partial L}{\\partial u}(x_{23})\\\\\n",
    "\\frac{\\partial L}{\\partial u}(x_{31}) & \\frac{\\partial L}{\\partial u}(x_{32}) & \\frac{\\partial L}{\\partial u}(x_{33})\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## MSE\n",
    "Mean squared error is a function of $u$.\n",
    "\n",
    "$$ MSE = \\frac{1}{n} \\sum_i (y_i - p_i)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
